---
title: "The Mathematics of Neural Networks"
image: "https:\/\/i.ytimg.com\/vi\/e5xKayCBOeU\/hqdefault.jpg"
vid_id: "e5xKayCBOeU"
categories: "Education"
tags: ["patterns","manifold hypothesis","neural networks"]
date: "2022-04-11T21:44:30+03:00"
vid_date: "2020-07-07T17:00:13Z"
duration: "PT14M22S"
viewcount: "102118"
likeCount: "4184"
dislikeCount: ""
channel: "Art of the Problem"
---
{% raw %}This video uses a spatial analogy to explore why deep neural networks are more powerful than shallow ones. This is part 4 in my deep learning series: <a rel="nofollow" target="blank" href="https://www.youtube.com/playlist?list=PLbg3ZX2pWlgKV8K6bFJr5dhM7oOClExUJ">https://www.youtube.com/playlist?list=PLbg3ZX2pWlgKV8K6bFJr5dhM7oOClExUJ</a> We'll explore what neurons are doing individually and as a group to &quot;understand&quot; perceptions. It leads us to the Manifold Hypothesis.{% endraw %}
