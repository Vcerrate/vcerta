---
title: "Categorical Distribution & Indicator Function | Intro | with TensorFlow Probability | [english]"
image: "https:\/\/i.ytimg.com\/vi\/421uW9aZHio\/hqdefault.jpg"
vid_id: "421uW9aZHio"
categories: "Education"
tags: ["tfp","multinulli","multinomial"]
date: "2022-01-17T14:44:15+03:00"
vid_date: "2021-03-29T19:45:34Z"
duration: "PT7M58S"
viewcount: "635"
likeCount: "18"
dislikeCount: ""
channel: "Machine Learning & Simulation"
---
{% raw %}Here are the notes: <a rel="nofollow" target="blank" href="https://raw.githubusercontent.com/Ceyron/machine-learning-and-simulation/main/english/essential_pmf_pdf/categorical_intro.pdf">https://raw.githubusercontent.com/Ceyron/machine-learning-and-simulation/main/english/essential_pmf_pdf/categorical_intro.pdf</a><br /><br />The Bernoulli Distribution allowed us to model discrete random variables with only two states (think of the weather which can either be good or bad). Often, we, however, want to model discrete variables with more than two states, maybe even multiple hundred. That is where the generalization of the Bernoulli, the Categorical, comes in. In this video we look at how to define such a kind of distribution and how to use it in TensorFlow Probability.<br /><br />Timestamps:<br />00:00 Intro<br />00:38 Encoding discrete states<br />01:09 Parameters of the Categorical<br />01:43 Important property of the parameters<br />02:06 Saving the last state's probability<br />02:50 Example Theta Array<br />03:12 The Probability Mass Function<br />03:49 The Indicator Function<br />04:09 Example<br />05:43 TFP: Setup<br />06:02 TFP: Using the Categorical<br />07:31 Outro{% endraw %}
